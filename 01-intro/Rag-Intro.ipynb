{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a749ea5",
   "metadata": {},
   "source": [
    "### Downloading the [MinSearch](https://github.com/alexeygrigorev/minsearch) Search Engine - a minimalistic text search engine that uses TF-IDF and cosine similarity for text fields and exact matching for keyword fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7673def6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-01 11:14:39--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5488 (5.4K) [text/plain]\n",
      "Saving to: ‘minsearch.py.3’\n",
      "\n",
      "minsearch.py.3      100%[===================>]   5.36K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-01 11:14:39 (57.1 MB/s) - ‘minsearch.py.3’ saved [5488/5488]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv --quiet\n",
    "!pip3 install tqdm notebook==7.1.2 openai elasticsearch==8.13.0 pandas scikit-learn ipywidgets --quiet\n",
    "\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch/minsearch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437b865",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries & Opening FAQ Parsed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f44c79-32e0-49cc-a207-0fa78123232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import minsearch\n",
    "from openai import OpenAI\n",
    "\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bf94d",
   "metadata": {},
   "source": [
    "### High-Level Summary\n",
    "This code flattens a nested list of FAQ documents. It takes a list of course-related document groups (`docs_raw`) and produces a single flat list (`documents`) where each FAQ entry includes its course name.\n",
    "\n",
    "#### Step-by-Step Breakdown\n",
    "Assume docs_raw looks like this (e.g., loaded from documents.json):\n",
    "\n",
    "```python\n",
    "docs_raw = [\n",
    "    {\n",
    "        'course': 'data-engineering-zoomcamp',\n",
    "        'documents': [\n",
    "            { 'section': 'Intro', 'question': 'What is DE?', 'text': '...' },\n",
    "            { 'section': 'Tools', 'question': 'What tools are used?', 'text': '...' }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'course': 'mlops-zoomcamp',\n",
    "        'documents': [\n",
    "            { 'section': 'Setup', 'question': 'How to set up?', 'text': '...' }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "```python\n",
    "### Initializes an empty list to hold all the individual FAQ entries across all courses.\n",
    "documents = []\n",
    "### Iterates over each course dictionary in docs_raw. Each course_dict contains a course name (course) & a list of FAQ documents for that course. \n",
    "for course_dict in docs_raw:\n",
    "    ### ### Iterates through each individual FAQ document (doc) in the current course’s documents list\n",
    "    for doc in course_dict['documents']:\n",
    "        ### Adds a new key 'course' to each doc so that each FAQ entry includes the course it belongs to.\n",
    "        doc['course'] = course_dict['course']\n",
    "        ### Adds a new key 'course' to each doc so that each FAQ entry includes the course it belongs to.\n",
    "        ### Appends the modified doc to the documents list.\n",
    "        documents.append(doc)\n",
    "```\n",
    "\n",
    "#### Final Result\n",
    "The resulting documents list is a flat list of all FAQ entries, each of which now includes the course name\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        'section': 'Intro',\n",
    "        'question': 'What is DE?',\n",
    "        'text': '...',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0b11a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Data Engineering Zoomcamp FAQ\\nData Engineering Zoomcamp FAQ\\nThe purpose of this document is to capture Frequently asked technical questions\\nEditing guidelines:\\nWhen adding a new FAQ entry, make sure the question is “Heading 2”\\nFeel free to improve if you see something is off\\nDon’t change the formatting in the Data document or add any visual “improvements” (make a copy for yourself first if you need to do it for whatever reason)\\nDon’t change the pages format (it should be “pageless”)\\nAdd name and date for reference, if possible\\nThe next cohort starts January 13th 2025. More info at DTC.\\nRegister before the course starts using this link.\\nJoint the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When does the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initializes an empty list to hold all the individual FAQ entries across all courses.\n",
    "documents = []\n",
    "### Iterates over each course dictionary in docs_raw. Each course_dict contains a course name (course) & a list of FAQ documents. \n",
    "for course_dict in docs_raw:\n",
    "    ### ### Iterates through each FAQ document (doc) in the current course’s documents list\n",
    "    for doc in course_dict['documents']:\n",
    "        ### Adds a new key 'course' to each doc so that each FAQ entry includes the course it belongs to.\n",
    "        doc['course'] = course_dict['course']\n",
    "        ### Adds a new key 'course' to each doc so that each FAQ entry includes the course it belongs to.\n",
    "        ### Appends the modified doc to the documents list.\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8e502",
   "metadata": {},
   "source": [
    "This code is initializing and fitting a **search index** using the `minsearch` library, which is likely designed for **semantic or keyword-based search** over structured documents (like FAQs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c433673d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x727b7aec8290>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "q = 'the course has already started, can I still enroll?'\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9a79cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "OPENAI_API_KEY = \"<ADD_YOUR_OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4ee4bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether you can still enroll in a course that has already started depends on several factors, such as the institution's policies, the specific course, and how far along the course is. Here are a few steps you can take:\\n\\n1. **Check the Institution's Guidelines**: Review the official website or contact the admissions office to understand their policy on late enrollments.\\n\\n2. **Contact the Instructor**: Often, instructors have some flexibility regarding late enrollments, especially if the course has just started. They might be willing to accommodate you.\\n\\n3. **Consider the Impact**: Think about how missing the start might affect your ability to catch up on missed content, assignments, and exams.\\n\\n4. **Online vs. In-Person Courses**: Online courses might offer more flexibility for late enrollment compared to traditional in-person classes.\\n\\n5. **Audit the Course**: If enrollment is not possible, ask if you can audit the course, allowing you to attend the classes without receiving credit.\\n\\nIf you're really interested in the course, it's worth reaching out to the relevant people to see what your options are.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from the `.env` file into the environment\n",
    "load_dotenv()\n",
    "\n",
    "# Access the key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6afd4",
   "metadata": {},
   "source": [
    "### High-Level Documentation\n",
    "This code implements a simple **Retrieval-Augmented Generation (RAG)** pipeline to answer user queries about the `\"data-engineering-zoomcamp\"` course. The process works as follows:\n",
    "1. **Search** relevant FAQ entries using a semantic search index (`minsearch`).\n",
    "2. **Build a prompt** by formatting the retrieved FAQs into a natural language instruction.\n",
    "3. **Query an LLM** (like GPT-4o) with that prompt to generate a precise, context-aware answer.\n",
    "4. **Return** the answer to the user.\n",
    "\n",
    "This is useful for building a course assistant that answers questions using only the information in a predefined FAQ knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece3b06f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query -> how do I run kafka?, and it's Response -> To run Kafka, you should ensure that all necessary components, including Docker images, are up and running first. If working in Python, create and activate a virtual environment to run the python files related to Kafka:\n",
      "\n",
      "1. Create and activate a virtual environment:\n",
      "   ```\n",
      "   python -m venv env\n",
      "   source env/bin/activate    # For MacOS/Linux\n",
      "   env\\Scripts\\activate       # For Windows\n",
      "   ```\n",
      "\n",
      "2. Install the necessary packages:\n",
      "   ```\n",
      "   pip install -r ../requirements.txt\n",
      "   ```\n",
      "\n",
      "3. To deactivate the virtual environment once you are done, use:\n",
      "   ```\n",
      "   deactivate\n",
      "   ```\n",
      "\n",
      "For running Java Kafka components (like producer/consumer/kstreams), use the following command in the terminal within the project directory:\n",
      "```\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "\n",
      "Query -> the course has already started, can I still enroll?, and it's Response -> Yes, you can still enroll in the course after it has started. You are eligible to submit the homework even if you don't register right away. Just keep in mind that there will be deadlines for submitting homework and final projects, so it's best not to leave everything until the last minute.\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    \"\"\"\n",
    "    Searches the minsearch index for relevant FAQ documents based on a user query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's natural language question.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top 5 matched documents from the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a boosting dictionary to prioritize certain fields more than others\n",
    "    boost = {\n",
    "        'question': 3.0,  # Give higher weight to matches in the 'question' field\n",
    "        'section': 0.5    # Give lower weight to matches in the 'section' field\n",
    "    }\n",
    "\n",
    "    # Perform the search using the index\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},  # Restrict to this course only\n",
    "        boost_dict=boost,     # Apply boost values to fields\n",
    "        num_results=5         # Return the top 5 matches\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    \"\"\"\n",
    "    Builds a prompt string for the LLM using retrieved FAQ entries as context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's natural language question.\n",
    "        search_results (list): A list of relevant documents returned by `search()`.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt to send to the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt template with placeholders for the question and context\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    # Loop over the retrieved documents and construct a context block\n",
    "    for doc in search_results:\n",
    "        # Append structured info from each document to the context\n",
    "        context += (\n",
    "            f\"section: {doc['section']}\\n\"\n",
    "            f\"question: {doc['question']}\\n\"\n",
    "            f\"answer: {doc['text']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # Fill in the prompt template with the user question and constructed context\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def llm(prompt):\n",
    "    \"\"\"\n",
    "    Sends the prompt to the OpenAI Chat API using GPT-4o and returns the model's response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): A complete natural language instruction including context and question.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the OpenAI Chat API with the GPT-4o model and a user message\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Extract the generated message text from the response object\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rag(query):\n",
    "    \"\"\"\n",
    "    Runs the full RAG (Retrieval-Augmented Generation) pipeline:\n",
    "    1. Retrieves relevant documents\n",
    "    2. Builds a prompt with them\n",
    "    3. Sends the prompt to the LLM\n",
    "    4. Returns the answer\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's input question.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Search relevant FAQ entries using the query\n",
    "    search_results = search(query)\n",
    "\n",
    "    # Step 2: Create a prompt using the matched documents\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    # Step 3: Generate a response from the LLM using the prompt\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "query_1 = 'how do I run kafka?'\n",
    "query_2 = 'the course has already started, can I still enroll?'\n",
    "\n",
    "print(f\"Query -> {query_1}, and it's Response -> {rag(query_1)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Query -> {query_2}, and it's Response -> {rag(query_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb2154",
   "metadata": {},
   "source": [
    "### Running ElasticSearch in Docker (code below)\n",
    "\n",
    "The code below launches `Elasticsearch 8.4.3` in **single-node mode**, disables security features, maps ports, and automatically removes the container when it exits. Useful for local development, testing, or short-term usage without the need for persistent storage or cluster setup.\n",
    "\n",
    "```bash\n",
    "# Run Elasticsearch in a Docker container\n",
    "docker run -it \\\n",
    "    # Automatically remove the container when it exits\n",
    "    --rm \\\n",
    "    # Assign the container a name for easier reference\n",
    "    --name elasticsearch \\\n",
    "    # Allocate 4GB of memory to the container (recommended for Elasticsearch)\n",
    "    -m 4GB \\\n",
    "    # Map port 9200 on host to 9200 in container (Elasticsearch REST API)\n",
    "    -p 9200:9200 \\\n",
    "    # Map port 9300 for internal cluster communication (not used in single-node)\n",
    "    -p 9300:9300 \\\n",
    "    # Run Elasticsearch in single-node mode (no clustering)\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    # Disable security features (no auth required — useful for local testing)\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    # Use the official Elasticsearch image (version 8.4.3)\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "```\n",
    "\n",
    "### Running the `ElasticSearch` in Docker and creating the index for search & retrieval\n",
    "\n",
    "This code sets up an **Elasticsearch** index using Python. It connects to a local Elasticsearch instance, defines how the index should be structured (settings + field types), and then creates the index.\n",
    "\n",
    "#### High-Level Summary\n",
    "- Connects to a local Elasticsearch instance (`http://localhost:9200`)\n",
    "- Defines an index called `course-questions`\n",
    "- Sets basic configuration for performance (e.g., 1 shard, 0 replicas)\n",
    "- Maps four fields: `text`, `section`, `question`, and `course` — with appropriate data types\n",
    "- Creates the index using the given schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7aa4379-320d-422b-ac9c-0dc21047da63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# The Python client to connect to an Elasticsearch cluster. Connects to an Elasticsearch server running locally on port 9200.\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1, # Splits the index into 1 shard (good for small datasets or local development).\n",
    "        \"number_of_replicas\": 0 # Sets 0 replicas (no redundancy, also okay for dev/test environments).\n",
    "    },\n",
    "    \"mappings\": { # Mappings: Define how each field in your documents will be indexed and queried. \n",
    "        # \"text\" fields are full-text searchable (analyzed into tokens).\n",
    "        # \"keyword\" fields (like course) are not analyzed — used for exact matches, filters, and aggregations.\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sets the name of the index to \"course-questions\".\n",
    "index_name = \"course-questions\"\n",
    "# Sends a request to the Elasticsearch server to create the index with the specified name and settings.\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n",
    "# After running this code: An index called course-questions will exist in your Elasticsearch instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3a16eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Data Engineering Zoomcamp FAQ\\nData Engineering Zoomcamp FAQ\\nThe purpose of this document is to capture Frequently asked technical questions\\nEditing guidelines:\\nWhen adding a new FAQ entry, make sure the question is “Heading 2”\\nFeel free to improve if you see something is off\\nDon’t change the formatting in the Data document or add any visual “improvements” (make a copy for yourself first if you need to do it for whatever reason)\\nDon’t change the pages format (it should be “pageless”)\\nAdd name and date for reference, if possible\\nThe next cohort starts January 13th 2025. More info at DTC.\\nRegister before the course starts using this link.\\nJoint the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When does the course start?', 'course': 'data-engineering-zoomcamp'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a8e3b5305648e396eb7f93ef40f187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Printing one document\n",
    "print(documents[0])\n",
    "\n",
    "## Indexing the documents in the ElasticSearch\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d84e13a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still join the course even if it has already started. You are eligible to submit the homework without registering. However, please be aware that there are deadlines for submitting homework and the final projects, so it's important not to leave everything until the last minute.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3 = 'I just disovered the course. Can I still join it?'\n",
    "\n",
    "def elastic_search(query):\n",
    "    # Construct the Elasticsearch query object\n",
    "    search_query = {\n",
    "        \"size\": 5,  # Limit the search to return only the top 5 results\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                # 'must' clause: defines conditions that must match\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,  # The user's input query string\n",
    "\n",
    "                        # Search across multiple fields in the index\n",
    "                        \"fields\": [\n",
    "                            \"question^3\",  # Boost the 'question' field by 3x to increase its weight in scoring\n",
    "                            \"text\",        # Search the main answer text\n",
    "                            \"section\"      # Also search section titles\n",
    "                        ],\n",
    "                        \"type\": \"best_fields\"  # Use the best matching field to score each document\n",
    "                    }\n",
    "                },\n",
    "                # 'filter' clause: restrict results without affecting score\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"  # Only include results from this specific course\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Send the query to Elasticsearch and store the response\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = []  # Prepare a list to collect the final matched documents\n",
    "    # Iterate over each hit in the search result\n",
    "    for hit in response['hits']['hits']:\n",
    "        # '_source' contains the actual content of the document\n",
    "        result_docs.append(hit['_source'])\n",
    "    # Return the list of matched documents (each is a dict with question, text, section, and course)\n",
    "    return result_docs\n",
    "\n",
    "def rag(query):\n",
    "    # Step 1: Search the FAQ index for relevant documents based on the input query.\n",
    "    # This uses full-text search with field boosts and filtering to find top matches.\n",
    "    search_results = elastic_search(query)\n",
    "\n",
    "    # Step 2: Construct a prompt for the language model.\n",
    "    # This combines the original user query and the context retrieved from the search results.\n",
    "    # The prompt follows a predefined format, instructing the LLM to answer only using the provided context.\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    # Step 3: Send the prompt to the LLM (e.g., GPT-4) and get the generated response.\n",
    "    # The model processes the question and context and returns an answer.\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    # Step 4: Return the model's response to the caller.\n",
    "    return answer\n",
    "\n",
    "\n",
    "rag(query_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
